# PIPELINE DEFINITION
# Name: github-rag-incremental-build
# Description: Incremental RAG pipeline for processing only changed GitHub files
# Inputs:
#    base_url: str [Default: 'https://www.kubeflow.org/docs']
#    changed_files: str [Default: '[]']
#    chunk_overlap: int [Default: 100.0]
#    chunk_size: int [Default: 1200.0]
#    collection_name: str [Default: 'docs_rag']
#    github_token: str [Default: '']
#    milvus_host: str [Default: 'milvus-standalone-final.santhosh.svc.cluster.local']
#    milvus_port: str [Default: '19530']
#    repo_name: str [Default: 'website']
#    repo_owner: str [Default: 'kubeflow']
components:
  comp-chunk-and-embed-incremental:
    executorLabel: exec-chunk-and-embed-incremental
    inputDefinitions:
      artifacts:
        github_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        base_url:
          parameterType: STRING
        chunk_overlap:
          parameterType: NUMBER_INTEGER
        chunk_size:
          parameterType: NUMBER_INTEGER
        repo_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        embedded_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-delete-old-vectors:
    executorLabel: exec-delete-old-vectors
    inputDefinitions:
      parameters:
        collection_name:
          parameterType: STRING
        file_paths:
          parameterType: STRING
        milvus_host:
          parameterType: STRING
        milvus_port:
          parameterType: STRING
        repo_name:
          parameterType: STRING
  comp-download-specific-files:
    executorLabel: exec-download-specific-files
    inputDefinitions:
      parameters:
        file_paths:
          parameterType: STRING
        github_token:
          parameterType: STRING
        repo_name:
          parameterType: STRING
        repo_owner:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        github_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-store-milvus-incremental:
    executorLabel: exec-store-milvus-incremental
    inputDefinitions:
      artifacts:
        embedded_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        collection_name:
          parameterType: STRING
        milvus_host:
          parameterType: STRING
        milvus_port:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-chunk-and-embed-incremental:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - chunk_and_embed_incremental
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'sentence-transformers'\
          \ 'langchain'  &&  python3 -m pip install --quiet --no-warn-script-location\
          \ 'kfp==2.14.3' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"\
          3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef chunk_and_embed_incremental(\n    github_data: dsl.Input[dsl.Dataset],\n\
          \    repo_name: str,\n    base_url: str,\n    chunk_size: int,\n    chunk_overlap:\
          \ int,\n    embedded_data: dsl.Output[dsl.Dataset]\n):\n    import json\n\
          \    import os\n    import re\n    import torch\n    from sentence_transformers\
          \ import SentenceTransformer\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n\
          \n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model\
          \ = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n\
          \    print(f\"Model loaded on {device}\")\n\n    records = []\n\n    with\
          \ open(github_data.path, 'r', encoding='utf-8') as f:\n        for line\
          \ in f:\n            file_data = json.loads(line)\n            content =\
          \ file_data['content']\n\n            # AGGRESSIVE CLEANING FOR BETTER EMBEDDINGS\
          \ (same as original)\n\n            # Remove Hugo frontmatter (both ---\
          \ and +++ styles)\n            content = re.sub(r'^\\s*[+\\-]{3,}.*?[+\\\
          -]{3,}\\s*', '', content, flags=re.DOTALL | re.MULTILINE)\n\n          \
          \  # Remove Hugo template syntax\n            content = re.sub(r'\\{\\{.*?\\\
          }\\}', '', content, flags=re.DOTALL)\n\n            # Remove HTML comments\
          \ and tags\n            content = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)\n\
          \            content = re.sub(r'<[^>]+>', ' ', content)\n\n            #\
          \ Remove navigation/menu artifacts\n            content = re.sub(r'\\b(Get\
          \ Started|Contribute|GenAI|Home|Menu|Navigation)\\b', '', content, flags=re.IGNORECASE)\n\
          \n            # Clean up URLs and links\n            content = re.sub(r'https?://[^\\\
          s]+', '', content)\n            content = re.sub(r'\\[([^\\]]+)\\]\\([^\\\
          )]+\\)', r'\\1', content)  # Convert [text](url) to text\n\n           \
          \ # Remove excessive whitespace and normalize\n            content = re.sub(r'\\\
          s+', ' ', content)  # Multiple spaces to single\n            content = re.sub(r'\\\
          n\\s*\\n\\s*\\n+', '\\n\\n', content)  # Multiple newlines to double\n \
          \           content = content.strip()\n\n            # Skip files that are\
          \ too short after cleaning\n            if len(content) < 50:\n        \
          \        print(f\"Skipping file after cleaning: {file_data['path']} ({len(content)}\
          \ chars)\")\n                continue\n\n            # Build citation URL\n\
          \            path_parts = file_data['path'].split('/')\n            if 'content/en/docs'\
          \ in file_data['path']:\n                docs_index = path_parts.index('docs')\n\
          \                url_path = '/'.join(path_parts[docs_index+1:])\n      \
          \          url_path = os.path.splitext(url_path)[0]\n                citation_url\
          \ = f\"{base_url}/{url_path}\"\n            else:\n                citation_url\
          \ = f\"{base_url}/{file_data['path']}\"\n\n            file_unique_id =\
          \ f\"{repo_name}:{file_data['path']}\"\n\n            # Create splitter\n\
          \            text_splitter = RecursiveCharacterTextSplitter(\n         \
          \       chunk_size=chunk_size,\n                chunk_overlap=chunk_overlap,\n\
          \                length_function=len,\n                separators=[\"\\\
          n\\n\", \"\\n\", \". \", \" \", \"\"]\n            )\n\n            # Split\
          \ into chunks\n            chunks = text_splitter.split_text(content)\n\n\
          \            print(f\"File: {file_data['path']} -> {len(chunks)} chunks\
          \ (avg: {sum(len(c) for c in chunks)/len(chunks):.0f} chars)\")\n\n    \
          \        # Create embeddings\n            for chunk_idx, chunk in enumerate(chunks):\n\
          \                embedding = model.encode(chunk).tolist()\n            \
          \    records.append({\n                    'file_unique_id': file_unique_id,\n\
          \                    'repo_name': repo_name,\n                    'file_path':\
          \ file_data['path'],\n                    'file_name': file_data['file_name'],\n\
          \                    'citation_url': citation_url[:1024],\n            \
          \        'chunk_index': chunk_idx,\n                    'content_text':\
          \ chunk[:2000],\n                    'embedding': embedding\n          \
          \      })\n\n    print(f\"Created {len(records)} total chunks for incremental\
          \ update\")\n\n    with open(embedded_data.path, 'w', encoding='utf-8')\
          \ as f:\n        for record in records:\n            f.write(json.dumps(record,\
          \ ensure_ascii=False) + '\\n')\n\n"
        image: pytorch/pytorch:2.3.0-cuda12.1-cudnn8-runtime
    exec-delete-old-vectors:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - delete_old_vectors
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'pymilvus'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.3'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef delete_old_vectors(\n    file_paths: str,  # JSON string of file\
          \ paths list\n    repo_name: str,\n    milvus_host: str,\n    milvus_port:\
          \ str,\n    collection_name: str\n):\n    from pymilvus import connections,\
          \ Collection\n    import json\n\n    # Connect to Milvus\n    connections.connect(\"\
          default\", host=milvus_host, port=milvus_port)\n\n    # Parse file paths\n\
          \    try:\n        file_paths_list = json.loads(file_paths)\n    except\
          \ json.JSONDecodeError:\n        print(f\"Error: Invalid JSON in file_paths:\
          \ {file_paths}\")\n        return\n\n    # Check if collection exists\n\
          \    try:\n        collection = Collection(collection_name)\n        collection.load()\n\
          \        print(f\"Connected to collection: {collection_name}\")\n\n    \
          \    # Delete old vectors for each changed file\n        deleted_count =\
          \ 0\n        for file_path in file_paths_list:\n            file_unique_id\
          \ = f\"{repo_name}:{file_path}\"\n\n            # Delete vectors with matching\
          \ file_unique_id\n            expr = f'file_unique_id == \"{file_unique_id}\"\
          '\n            try:\n                # Get count before deletion for logging\n\
          \                query_result = collection.query(\n                    expr=expr,\n\
          \                    output_fields=[\"id\"],\n                    limit=10000\n\
          \                )\n                count_before = len(query_result)\n\n\
          \                if count_before > 0:\n                    # Delete the\
          \ vectors\n                    collection.delete(expr)\n               \
          \     collection.flush()\n                    deleted_count += count_before\n\
          \                    print(f\"Deleted {count_before} vectors for file: {file_path}\"\
          )\n                else:\n                    print(f\"No existing vectors\
          \ found for file: {file_path}\")\n\n            except Exception as e:\n\
          \                print(f\"Error deleting vectors for {file_path}: {e}\"\
          )\n                continue\n\n        print(f\"\u2705 Total deleted vectors:\
          \ {deleted_count}\")\n\n    except Exception as e:\n        print(f\"Error\
          \ connecting to collection {collection_name}: {e}\")\n        print(\"Collection\
          \ might not exist yet - this is okay for first run\")\n\n"
        image: python:3.9
    exec-download-specific-files:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_specific_files
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'requests' 'beautifulsoup4'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.3'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_specific_files(\n    repo_owner: str,\n    repo_name:\
          \ str,\n    file_paths: str,  # JSON string of file paths list\n    github_token:\
          \ str,\n    github_data: dsl.Output[dsl.Dataset]\n):\n    import requests\n\
          \    import json\n    import base64\n    from bs4 import BeautifulSoup\n\
          \n    headers = {\"Authorization\": f\"token {github_token}\"} if github_token\
          \ else {}\n\n    # Parse the file paths from JSON string\n    try:\n   \
          \     file_paths_list = json.loads(file_paths)\n    except json.JSONDecodeError:\n\
          \        print(f\"Error: Invalid JSON in file_paths: {file_paths}\")\n \
          \       file_paths_list = []\n\n    print(f\"Processing {len(file_paths_list)}\
          \ changed files\")\n\n    files = []\n\n    for file_path in file_paths_list:\n\
          \        # Skip non-documentation files\n        if not (file_path.endswith('.md')\
          \ or file_path.endswith('.html')):\n            print(f\"Skipping non-doc\
          \ file: {file_path}\")\n            continue\n\n        try:\n         \
          \   # Get file content from GitHub API\n            api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{file_path}\"\
          \n            response = requests.get(api_url, headers=headers)\n      \
          \      response.raise_for_status()\n            file_data = response.json()\n\
          \n            # Decode content\n            content = base64.b64decode(file_data['content']).decode('utf-8')\n\
          \n            # Extract text from HTML files\n            if file_path.endswith('.html'):\n\
          \                soup = BeautifulSoup(content, 'html.parser')\n        \
          \        content = soup.get_text(separator=' ', strip=True)\n\n        \
          \    files.append({\n                'path': file_path,\n              \
          \  'content': content,\n                'file_name': file_data['name']\n\
          \            })\n            print(f\"Downloaded: {file_path}\")\n\n   \
          \     except Exception as e:\n            print(f\"Error downloading {file_path}:\
          \ {e}\")\n            continue\n\n    print(f\"Successfully downloaded {len(files)}\
          \ files\")\n\n    # Save to output dataset\n    with open(github_data.path,\
          \ 'w', encoding='utf-8') as f:\n        for file_data in files:\n      \
          \      f.write(json.dumps(file_data, ensure_ascii=False) + '\\n')\n\n"
        image: python:3.9
    exec-store-milvus-incremental:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - store_milvus_incremental
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'pymilvus' 'numpy'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.3'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef store_milvus_incremental(\n    embedded_data: dsl.Input[dsl.Dataset],\n\
          \    milvus_host: str,\n    milvus_port: str,\n    collection_name: str\n\
          ):\n    from pymilvus import connections, utility, FieldSchema, CollectionSchema,\
          \ DataType, Collection\n    import json\n    from datetime import datetime\n\
          \n    connections.connect(\"default\", host=milvus_host, port=milvus_port)\n\
          \n    # Check if collection exists, if not create it\n    if not utility.has_collection(collection_name):\n\
          \        print(f\"Collection {collection_name} doesn't exist, creating it...\"\
          )\n\n        # Enhanced schema with 768 dimensions\n        fields = [\n\
          \            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True,\
          \ auto_id=True),\n            FieldSchema(name=\"file_unique_id\", dtype=DataType.VARCHAR,\
          \ max_length=512),\n            FieldSchema(name=\"repo_name\", dtype=DataType.VARCHAR,\
          \ max_length=256),\n            FieldSchema(name=\"file_path\", dtype=DataType.VARCHAR,\
          \ max_length=512),\n            FieldSchema(name=\"file_name\", dtype=DataType.VARCHAR,\
          \ max_length=256),\n            FieldSchema(name=\"citation_url\", dtype=DataType.VARCHAR,\
          \ max_length=1024),\n            FieldSchema(name=\"chunk_index\", dtype=DataType.INT64),\n\
          \            FieldSchema(name=\"content_text\", dtype=DataType.VARCHAR,\
          \ max_length=2000),\n            FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR,\
          \ dim=768),\n            FieldSchema(name=\"last_updated\", dtype=DataType.INT64)\n\
          \        ]\n\n        schema = CollectionSchema(fields, \"RAG collection\
          \ for documentation\")\n        collection = Collection(collection_name,\
          \ schema)\n        print(f\"Created new collection: {collection_name}\"\
          )\n    else:\n        collection = Collection(collection_name)\n       \
          \ print(f\"Using existing collection: {collection_name}\")\n\n    # Load\
          \ collection\n    collection.load()\n\n    # Prepare records for insertion\n\
          \    records = []\n    timestamp = int(datetime.now().timestamp())\n\n \
          \   with open(embedded_data.path, 'r', encoding='utf-8') as f:\n       \
          \ for line in f:\n            record = json.loads(line)\n            records.append({\n\
          \                \"file_unique_id\": record[\"file_unique_id\"],\n     \
          \           \"repo_name\": record[\"repo_name\"],\n                \"file_path\"\
          : record[\"file_path\"],\n                \"file_name\": record[\"file_name\"\
          ],\n                \"citation_url\": record[\"citation_url\"],\n      \
          \          \"chunk_index\": record[\"chunk_index\"],\n                \"\
          content_text\": record[\"content_text\"],\n                \"vector\": record[\"\
          embedding\"],\n                \"last_updated\": timestamp\n           \
          \ })\n\n    if records:\n        # Insert new records\n        batch_size\
          \ = 1000\n        for i in range(0, len(records), batch_size):\n       \
          \     batch = records[i:i + batch_size]\n            collection.insert(batch)\n\
          \n        collection.flush()\n\n        # Create/update index if needed\n\
          \        try:\n            # Check if index exists\n            index_info\
          \ = collection.index()\n            if not index_info:\n               \
          \ print(\"Creating index...\")\n                index_params = {\n     \
          \               \"metric_type\": \"COSINE\",\n                    \"index_type\"\
          : \"IVF_FLAT\", \n                    \"params\": {\"nlist\": min(1024,\
          \ max(100, len(records)))}\n                }\n                collection.create_index(\"\
          vector\", index_params)\n                collection.load()\n           \
          \     print(\"Index created successfully\")\n            else:\n       \
          \         print(\"Index already exists\")\n        except Exception as e:\n\
          \            print(f\"Index operation result: {e}\")\n\n        print(f\"\
          \u2705 Inserted {len(records)} new records. Total collection size: {collection.num_entities}\"\
          )\n    else:\n        print(\"No records to insert\")\n\n"
        image: python:3.9
pipelineInfo:
  description: Incremental RAG pipeline for processing only changed GitHub files
  name: github-rag-incremental-build
root:
  dag:
    tasks:
      chunk-and-embed-incremental:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-chunk-and-embed-incremental
        dependentTasks:
        - download-specific-files
        inputs:
          artifacts:
            github_data:
              taskOutputArtifact:
                outputArtifactKey: github_data
                producerTask: download-specific-files
          parameters:
            base_url:
              componentInputParameter: base_url
            chunk_overlap:
              componentInputParameter: chunk_overlap
            chunk_size:
              componentInputParameter: chunk_size
            repo_name:
              componentInputParameter: repo_name
        taskInfo:
          name: chunk-and-embed-incremental
      delete-old-vectors:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-delete-old-vectors
        inputs:
          parameters:
            collection_name:
              componentInputParameter: collection_name
            file_paths:
              componentInputParameter: changed_files
            milvus_host:
              componentInputParameter: milvus_host
            milvus_port:
              componentInputParameter: milvus_port
            repo_name:
              componentInputParameter: repo_name
        taskInfo:
          name: delete-old-vectors
      download-specific-files:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-specific-files
        inputs:
          parameters:
            file_paths:
              componentInputParameter: changed_files
            github_token:
              componentInputParameter: github_token
            repo_name:
              componentInputParameter: repo_name
            repo_owner:
              componentInputParameter: repo_owner
        taskInfo:
          name: download-specific-files
      store-milvus-incremental:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-store-milvus-incremental
        dependentTasks:
        - chunk-and-embed-incremental
        - delete-old-vectors
        inputs:
          artifacts:
            embedded_data:
              taskOutputArtifact:
                outputArtifactKey: embedded_data
                producerTask: chunk-and-embed-incremental
          parameters:
            collection_name:
              componentInputParameter: collection_name
            milvus_host:
              componentInputParameter: milvus_host
            milvus_port:
              componentInputParameter: milvus_port
        taskInfo:
          name: store-milvus-incremental
  inputDefinitions:
    parameters:
      base_url:
        defaultValue: https://www.kubeflow.org/docs
        isOptional: true
        parameterType: STRING
      changed_files:
        defaultValue: '[]'
        isOptional: true
        parameterType: STRING
      chunk_overlap:
        defaultValue: 100.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      chunk_size:
        defaultValue: 1200.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      collection_name:
        defaultValue: docs_rag
        isOptional: true
        parameterType: STRING
      github_token:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      milvus_host:
        defaultValue: milvus-standalone-final.santhosh.svc.cluster.local
        isOptional: true
        parameterType: STRING
      milvus_port:
        defaultValue: '19530'
        isOptional: true
        parameterType: STRING
      repo_name:
        defaultValue: website
        isOptional: true
        parameterType: STRING
      repo_owner:
        defaultValue: kubeflow
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.14.3
