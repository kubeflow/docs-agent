# PIPELINE DEFINITION
# Name: github-rag-full-build
# Description: RAG pipeline for processing GitHub documentation
# Inputs:
#    base_url: str [Default: 'https://www.kubeflow.org/docs']
#    chunk_overlap: int [Default: 100.0]
#    chunk_size: int [Default: 1000.0]
#    collection_name: str [Default: 'docs_rag']
#    directory_path: str [Default: 'content/en']
#    github_token: str [Default: '']
#    milvus_host: str [Default: 'milvus-standalone-final.docs-agent.svc.cluster.local']
#    milvus_port: str [Default: '19530']
#    repo_name: str [Default: 'website']
#    repo_owner: str [Default: 'kubeflow']
components:
  comp-chunk-and-embed:
    executorLabel: exec-chunk-and-embed
    inputDefinitions:
      artifacts:
        github_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        base_url:
          parameterType: STRING
        chunk_overlap:
          parameterType: NUMBER_INTEGER
        chunk_size:
          parameterType: NUMBER_INTEGER
        repo_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        embedded_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-download-github-directory:
    executorLabel: exec-download-github-directory
    inputDefinitions:
      parameters:
        directory_path:
          parameterType: STRING
        github_token:
          parameterType: STRING
        repo_name:
          parameterType: STRING
        repo_owner:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        github_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-store-milvus:
    executorLabel: exec-store-milvus
    inputDefinitions:
      artifacts:
        embedded_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        collection_name:
          parameterType: STRING
        milvus_host:
          parameterType: STRING
        milvus_port:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-chunk-and-embed:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - chunk_and_embed
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'sentence-transformers'\
          \ 'langchain'  &&  python3 -m pip install --quiet --no-warn-script-location\
          \ 'kfp==2.14.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"\
          3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef chunk_and_embed(\n    github_data: dsl.Input[dsl.Dataset],\n\
          \    repo_name: str,\n    base_url: str,\n    chunk_size: int,\n    chunk_overlap:\
          \ int,\n    embedded_data: dsl.Output[dsl.Dataset]\n):\n    import json\n\
          \    import os\n    import re\n    import torch\n    from sentence_transformers\
          \ import SentenceTransformer\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n\
          \n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model\
          \ = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n\
          \    print(f\"Model loaded on {device}\")\n\n    records = []\n\n    with\
          \ open(github_data.path, 'r', encoding='utf-8') as f:\n        for line\
          \ in f:\n            file_data = json.loads(line)\n            content =\
          \ file_data['content']\n\n            # AGGRESSIVE CLEANING FOR BETTER EMBEDDINGS\n\
          \n            # Remove Hugo frontmatter (both --- and +++ styles)\n    \
          \        content = re.sub(r'^\\s*[+\\-]{3,}.*?[+\\-]{3,}\\s*', '', content,\
          \ flags=re.DOTALL | re.MULTILINE)\n\n            # Remove Hugo template\
          \ syntax\n            content = re.sub(r'\\{\\{.*?\\}\\}', '', content,\
          \ flags=re.DOTALL)\n\n            # Remove HTML comments and tags\n    \
          \        content = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)\n\
          \            content = re.sub(r'<[^>]+>', ' ', content)\n\n            #\
          \ Remove navigation/menu artifacts\n            content = re.sub(r'\\b(Get\
          \ Started|Contribute|GenAI|Home|Menu|Navigation)\\b', '', content, flags=re.IGNORECASE)\n\
          \n            # Clean up URLs and links\n            content = re.sub(r'https?://[^\\\
          s]+', '', content)\n            content = re.sub(r'\\[([^\\]]+)\\]\\([^\\\
          )]+\\)', r'\\1', content)  # Convert [text](url) to text\n\n           \
          \ # Remove excessive whitespace and normalize\n            content = re.sub(r'\\\
          s+', ' ', content)  # Multiple spaces to single\n            content = re.sub(r'\\\
          n\\s*\\n\\s*\\n+', '\\n\\n', content)  # Multiple newlines to double\n \
          \           content = content.strip()\n\n            # Skip files that are\
          \ too short after cleaning\n            if len(content) < 50:\n        \
          \        print(f\"Skipping file after cleaning: {file_data['path']} ({len(content)}\
          \ chars)\")\n                continue\n\n            # Build citation URL\
          \ (same as before)\n            path_parts = file_data['path'].split('/')\n\
          \            if 'content/en/docs' in file_data['path']:\n              \
          \  docs_index = path_parts.index('docs')\n                url_path = '/'.join(path_parts[docs_index+1:])\n\
          \                url_path = os.path.splitext(url_path)[0]\n            \
          \    citation_url = f\"{base_url}/{url_path}\"\n            else:\n    \
          \            citation_url = f\"{base_url}/{file_data['path']}\"\n\n    \
          \        file_unique_id = f\"{repo_name}:{file_data['path']}\"\n\n     \
          \       # Create splitter\n            text_splitter = RecursiveCharacterTextSplitter(\n\
          \                chunk_size=chunk_size,\n                chunk_overlap=chunk_overlap,\n\
          \                length_function=len,\n                separators=[\"\\\
          n\\n\", \"\\n\", \". \", \" \", \"\"]\n            )\n\n            # Split\
          \ into chunks\n            chunks = text_splitter.split_text(content)\n\n\
          \            print(f\"File: {file_data['path']} -> {len(chunks)} chunks\
          \ (avg: {sum(len(c) for c in chunks)/len(chunks):.0f} chars)\")\n\n    \
          \        # Create embeddings\n            for chunk_idx, chunk in enumerate(chunks):\n\
          \                embedding = model.encode(chunk).tolist()\n            \
          \    records.append({\n                    'file_unique_id': file_unique_id,\n\
          \                    'repo_name': repo_name,\n                    'file_path':\
          \ file_data['path'],\n                    'file_name': file_data['file_name'],\n\
          \                    'citation_url': citation_url[:1024],\n            \
          \        'chunk_index': chunk_idx,\n                    'content_text':\
          \ chunk[:2000],\n                    'embedding': embedding\n          \
          \      })\n\n    print(f\"Created {len(records)} total chunks\")\n\n   \
          \ with open(embedded_data.path, 'w', encoding='utf-8') as f:\n        for\
          \ record in records:\n            f.write(json.dumps(record, ensure_ascii=False)\
          \ + '\\n')\n\n"
        image: pytorch/pytorch:2.3.0-cuda12.1-cudnn8-runtime
    exec-download-github-directory:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_github_directory
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'requests' 'beautifulsoup4'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_github_directory(\n    repo_owner: str,\n    repo_name:\
          \ str,\n    directory_path: str,\n    github_token: str,\n    github_data:\
          \ dsl.Output[dsl.Dataset]\n):\n    import requests\n    import json\n  \
          \  import base64\n    from bs4 import BeautifulSoup\n\n    headers = {\"\
          Authorization\": f\"token {github_token}\"} if github_token else {}\n  \
          \  api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{directory_path}\"\
          \n\n    def get_files_recursive(url):\n        files = []\n        try:\n\
          \            response = requests.get(url, headers=headers)\n           \
          \ response.raise_for_status()\n            items = response.json()\n\n \
          \           for item in items:\n                if item['type'] == 'file'\
          \ and (item['name'].endswith('.md') or item['name'].endswith('.html')):\n\
          \                    file_response = requests.get(item['url'], headers=headers)\n\
          \                    file_response.raise_for_status()\n                \
          \    file_data = file_response.json()\n                    content = base64.b64decode(file_data['content']).decode('utf-8')\n\
          \n                    # Extract text from HTML files\n                 \
          \   if item['name'].endswith('.html'):\n                        soup = BeautifulSoup(content,\
          \ 'html.parser')\n                        content = soup.get_text(separator='\
          \ ', strip=True)\n\n                    files.append({\n               \
          \         'path': item['path'],\n                        'content': content,\n\
          \                        'file_name': item['name']\n                   \
          \ })\n                elif item['type'] == 'dir':\n                    files.extend(get_files_recursive(item['url']))\n\
          \        except Exception as e:\n            print(f\"Error fetching {url}:\
          \ {e}\")\n        return files\n\n    files = get_files_recursive(api_url)\n\
          \    print(f\"Downloaded {len(files)} files\")\n\n    with open(github_data.path,\
          \ 'w', encoding='utf-8') as f:\n        for file_data in files:\n      \
          \      f.write(json.dumps(file_data, ensure_ascii=False) + '\\n')\n\n"
        image: python:3.9
    exec-store-milvus:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - store_milvus
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'pymilvus' 'numpy'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef store_milvus(\n    embedded_data: dsl.Input[dsl.Dataset],\n \
          \   milvus_host: str,\n    milvus_port: str,\n    collection_name: str\n\
          ):\n    from pymilvus import connections, utility, FieldSchema, CollectionSchema,\
          \ DataType, Collection\n    import json\n    from datetime import datetime\n\
          \n    connections.connect(\"default\", host=milvus_host, port=milvus_port)\n\
          \n    # DROP existing collection to fix schema mismatch\n    if utility.has_collection(collection_name):\n\
          \        utility.drop_collection(collection_name)\n        print(f\"Dropped\
          \ existing collection: {collection_name}\")\n\n    # Enhanced schema with\
          \ 768 dimensions\n    fields = [\n        FieldSchema(name=\"id\", dtype=DataType.INT64,\
          \ is_primary=True, auto_id=True),\n        FieldSchema(name=\"file_unique_id\"\
          , dtype=DataType.VARCHAR, max_length=512),\n        FieldSchema(name=\"\
          repo_name\", dtype=DataType.VARCHAR, max_length=256),\n        FieldSchema(name=\"\
          file_path\", dtype=DataType.VARCHAR, max_length=512),\n        FieldSchema(name=\"\
          file_name\", dtype=DataType.VARCHAR, max_length=256),\n        FieldSchema(name=\"\
          citation_url\", dtype=DataType.VARCHAR, max_length=1024),\n        FieldSchema(name=\"\
          chunk_index\", dtype=DataType.INT64),\n        FieldSchema(name=\"content_text\"\
          , dtype=DataType.VARCHAR, max_length=2000),\n        FieldSchema(name=\"\
          vector\", dtype=DataType.FLOAT_VECTOR, dim=768),  # Updated for all-mpnet-base-v2\n\
          \        FieldSchema(name=\"last_updated\", dtype=DataType.INT64)\n    ]\n\
          \n    # Create new collection with correct schema\n    schema = CollectionSchema(fields,\
          \ \"RAG collection for documentation\")\n    collection = Collection(collection_name,\
          \ schema)\n    print(f\"Created new collection: {collection_name}\")\n\n\
          \    # Rest of your existing code remains the same...\n    records = []\n\
          \    timestamp = int(datetime.now().timestamp())\n\n    with open(embedded_data.path,\
          \ 'r', encoding='utf-8') as f:\n        for line in f:\n            record\
          \ = json.loads(line)\n            records.append({\n                \"file_unique_id\"\
          : record[\"file_unique_id\"],\n                \"repo_name\": record[\"\
          repo_name\"],\n                \"file_path\": record[\"file_path\"],\n \
          \               \"file_name\": record[\"file_name\"],\n                \"\
          citation_url\": record[\"citation_url\"],\n                \"chunk_index\"\
          : record[\"chunk_index\"],\n                \"content_text\": record[\"\
          content_text\"],\n                \"vector\": record[\"embedding\"],\n \
          \               \"last_updated\": timestamp\n            })\n\n    if records:\n\
          \        batch_size = 1000\n        for i in range(0, len(records), batch_size):\n\
          \            batch = records[i:i + batch_size]\n            collection.insert(batch)\n\
          \n        collection.flush()\n\n        # Create index\n        index_params\
          \ = {\n            \"metric_type\": \"COSINE\",\n            \"index_type\"\
          : \"IVF_FLAT\", \n            \"params\": {\"nlist\": min(1024, len(records))}\n\
          \        }\n        collection.create_index(\"vector\", index_params)\n\
          \        collection.load()\n        print(f\"\u2705 Inserted {len(records)}\
          \ records. Total: {collection.num_entities}\")\n\n"
        image: python:3.9
pipelineInfo:
  description: RAG pipeline for processing GitHub documentation
  name: github-rag-full-build
root:
  dag:
    tasks:
      chunk-and-embed:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-chunk-and-embed
        dependentTasks:
        - download-github-directory
        inputs:
          artifacts:
            github_data:
              taskOutputArtifact:
                outputArtifactKey: github_data
                producerTask: download-github-directory
          parameters:
            base_url:
              componentInputParameter: base_url
            chunk_overlap:
              componentInputParameter: chunk_overlap
            chunk_size:
              componentInputParameter: chunk_size
            repo_name:
              componentInputParameter: repo_name
        taskInfo:
          name: chunk-and-embed
      download-github-directory:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-github-directory
        inputs:
          parameters:
            directory_path:
              componentInputParameter: directory_path
            github_token:
              componentInputParameter: github_token
            repo_name:
              componentInputParameter: repo_name
            repo_owner:
              componentInputParameter: repo_owner
        taskInfo:
          name: download-github-directory
      store-milvus:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-store-milvus
        dependentTasks:
        - chunk-and-embed
        inputs:
          artifacts:
            embedded_data:
              taskOutputArtifact:
                outputArtifactKey: embedded_data
                producerTask: chunk-and-embed
          parameters:
            collection_name:
              componentInputParameter: collection_name
            milvus_host:
              componentInputParameter: milvus_host
            milvus_port:
              componentInputParameter: milvus_port
        taskInfo:
          name: store-milvus
  inputDefinitions:
    parameters:
      base_url:
        defaultValue: https://www.kubeflow.org/docs
        isOptional: true
        parameterType: STRING
      chunk_overlap:
        defaultValue: 100.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      chunk_size:
        defaultValue: 1000.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      collection_name:
        defaultValue: docs_rag
        isOptional: true
        parameterType: STRING
      directory_path:
        defaultValue: content/en
        isOptional: true
        parameterType: STRING
      github_token:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      milvus_host:
        defaultValue: milvus-standalone-final.docs-agent.svc.cluster.local
        isOptional: true
        parameterType: STRING
      milvus_port:
        defaultValue: '19530'
        isOptional: true
        parameterType: STRING
      repo_name:
        defaultValue: website
        isOptional: true
        parameterType: STRING
      repo_owner:
        defaultValue: kubeflow
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.14.2
